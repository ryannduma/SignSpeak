{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# SignSpeak: Sign Language Data Exploration\n",
       "\n",
       "This notebook explores sign language datasets and extracts key features for training the sign recognition model."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Import libraries\n",
       "import os\n",
       "import sys\n",
       "import cv2\n",
       "import numpy as np\n",
       "import pandas as pd\n",
       "import matplotlib.pyplot as plt\n",
       "import mediapipe as mp\n",
       "from tqdm.notebook import tqdm\n",
       "\n",
       "# Add src directory to path\n",
       "sys.path.append(os.path.abspath('../src'))\n",
       "\n",
       "# Import SignSpeak modules\n",
       "from sign_recognition.detector import SignDetector"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Setting Up MediaPipe\n",
       "\n",
       "We'll use MediaPipe Holistic for pose, face, and hand landmark detection."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Initialize detector\n",
       "detector = SignDetector()\n",
       "\n",
       "# Test with a sample image\n",
       "sample_image = np.zeros((480, 640, 3), dtype=np.uint8)\n",
       "plt.figure(figsize=(10, 6))\n",
       "plt.imshow(cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB))\n",
       "plt.title('Sample Black Image')\n",
       "plt.axis('off')\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Loading and Exploring Sign Language Datasets\n",
       "\n",
       "In this section, we would download and explore sign language datasets. For this example, let's imagine we have access to a dataset of sign language videos."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Define helper function to list available datasets\n",
       "def list_available_datasets():\n",
       "    \"\"\"List available sign language datasets.\"\"\"\n",
       "    datasets = {\n",
       "        \"ASLLVD\": {\n",
       "            \"description\": \"American Sign Language Lexicon Video Dataset\",\n",
       "            \"url\": \"https://www.bu.edu/asllrp/av/dai-asllvd.html\",\n",
       "            \"size\": \"9,794 videos of 3,000+ signs\"\n",
       "        },\n",
       "        \"MSASL\": {\n",
       "            \"description\": \"Microsoft American Sign Language Dataset\",\n",
       "            \"url\": \"https://www.microsoft.com/en-us/download/details.aspx?id=100121\",\n",
       "            \"size\": \"25,000+ videos of 1,000 signs\"\n",
       "        },\n",
       "        \"WLASL\": {\n",
       "            \"description\": \"Word-Level American Sign Language Dataset\",\n",
       "            \"url\": \"https://github.com/dxli94/WLASL\",\n",
       "            \"size\": \"21,000+ videos of 2,000 signs\"\n",
       "        }\n",
       "    }\n",
       "    \n",
       "    print(\"Available Sign Language Datasets:\")\n",
       "    for name, info in datasets.items():\n",
       "        print(f\"\\n{name}:\")\n",
       "        print(f\"  Description: {info['description']}\")\n",
       "        print(f\"  URL: {info['url']}\")\n",
       "        print(f\"  Size: {info['size']}\")\n",
       "    \n",
       "    return datasets\n",
       "\n",
       "# List available datasets\n",
       "available_datasets = list_available_datasets()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Processing Video Data\n",
       "\n",
       "Here we'll create a function to process video files and extract landmarks using MediaPipe."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def process_video(video_path, max_frames=None):\n",
       "    \"\"\"Process a video file and extract landmarks using MediaPipe.\n",
       "    \n",
       "    Args:\n",
       "        video_path: Path to the video file\n",
       "        max_frames: Maximum number of frames to process (None = all frames)\n",
       "        \n",
       "    Returns:\n",
       "        A list of keypoint dictionaries (one per frame)\n",
       "    \"\"\"\n",
       "    # Create video capture object\n",
       "    cap = cv2.VideoCapture(video_path)\n",
       "    if not cap.isOpened():\n",
       "        print(f\"Error: Could not open video file {video_path}\")\n",
       "        return None\n",
       "    \n",
       "    # Get video properties\n",
       "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
       "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
       "    duration = frame_count / fps\n",
       "    \n",
       "    print(f\"Video info: {frame_count} frames, {fps:.2f} FPS, {duration:.2f} seconds\")\n",
       "    \n",
       "    # Limit frames if specified\n",
       "    if max_frames is not None and max_frames < frame_count:\n",
       "        frame_count = max_frames\n",
       "    \n",
       "    # Process frames\n",
       "    keypoints_sequence = []\n",
       "    for i in tqdm(range(frame_count)):\n",
       "        ret, frame = cap.read()\n",
       "        if not ret:\n",
       "            break\n",
       "        \n",
       "        # Process frame using our detector\n",
       "        _, keypoints = detector.process_frame(frame)\n",
       "        \n",
       "        # Store keypoints\n",
       "        if keypoints is not None:\n",
       "            keypoints_sequence.append(keypoints)\n",
       "        else:\n",
       "            # If no keypoints detected, store an empty dict\n",
       "            keypoints_sequence.append({})\n",
       "    \n",
       "    # Release video capture\n",
       "    cap.release()\n",
       "    \n",
       "    return keypoints_sequence\n",
       "\n",
       "# For demo purposes, we'll just print a message since we don't have a video file available\n",
       "print(\"To process a video, call process_video(video_path)\")\n",
       "print(\"For example: keypoints = process_video('path/to/sign_video.mp4', max_frames=100)\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Data Visualization\n",
       "\n",
       "Let's create functions to visualize the extracted keypoints."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def visualize_keypoints(keypoints, figsize=(15, 10)):\n",
       "    \"\"\"Visualize keypoints in 3D space.\n",
       "    \n",
       "    Args:\n",
       "        keypoints: Dictionary of keypoints from the SignDetector\n",
       "        figsize: Figure size (width, height)\n",
       "    \"\"\"\n",
       "    if not keypoints or not any(keypoints.values()):\n",
       "        print(\"No keypoints to visualize.\")\n",
       "        return\n",
       "    \n",
       "    fig = plt.figure(figsize=figsize)\n",
       "    ax = fig.add_subplot(111, projection='3d')\n",
       "    \n",
       "    # Plot keypoints by type\n",
       "    colors = {\n",
       "        'face': 'green',\n",
       "        'pose': 'blue',\n",
       "        'left_hand': 'red',\n",
       "        'right_hand': 'purple'\n",
       "    }\n",
       "    \n",
       "    for part_name, part_keypoints in keypoints.items():\n",
       "        if part_name in colors and np.any(part_keypoints):\n",
       "            x = part_keypoints[:, 0]\n",
       "            y = part_keypoints[:, 1]\n",
       "            z = part_keypoints[:, 2]\n",
       "            ax.scatter(x, y, z, c=colors[part_name], marker='o', label=part_name)\n",
       "    \n",
       "    # Set labels and title\n",
       "    ax.set_xlabel('X')\n",
       "    ax.set_ylabel('Y')\n",
       "    ax.set_zlabel('Z')\n",
       "    ax.set_title('Keypoints Visualization')\n",
       "    \n",
       "    # Set axis limits (MediaPipe gives normalized coordinates)\n",
       "    ax.set_xlim(0, 1)\n",
       "    ax.set_ylim(0, 1)\n",
       "    ax.set_zlim(-0.2, 0.2)\n",
       "    \n",
       "    # Add legend\n",
       "    ax.legend()\n",
       "    \n",
       "    plt.show()\n",
       "\n",
       "# Example keypoints for visualization (placeholder data)\n",
       "example_keypoints = {\n",
       "    'face': np.array([[0.5, 0.2, 0]] * 10),  # Face keypoints\n",
       "    'pose': np.array([[0.5, 0.5, 0]] * 10),  # Pose keypoints\n",
       "    'left_hand': np.array([[0.3, 0.6, 0]] * 10),  # Left hand keypoints\n",
       "    'right_hand': np.array([[0.7, 0.6, 0]] * 10)  # Right hand keypoints\n",
       "}\n",
       "\n",
       "# Visualize example keypoints\n",
       "visualize_keypoints(example_keypoints)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Feature Extraction for Model Training\n",
       "\n",
       "Let's implement functions to extract features for training our sign recognition model."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def extract_features(keypoints_sequence):\n",
       "    \"\"\"Extract features from keypoints sequence for model training.\n",
       "    \n",
       "    Args:\n",
       "        keypoints_sequence: List of keypoint dictionaries\n",
       "        \n",
       "    Returns:\n",
       "        Numpy array of features\n",
       "    \"\"\"\n",
       "    if not keypoints_sequence:\n",
       "        return None\n",
       "    \n",
       "    # Initialize feature list\n",
       "    features = []\n",
       "    \n",
       "    # Process each frame in the sequence\n",
       "    for keypoints in keypoints_sequence:\n",
       "        if not keypoints:  # Skip empty frames\n",
       "            continue\n",
       "        \n",
       "        # Extract features from each part\n",
       "        frame_features = []\n",
       "        \n",
       "        # Process hands (most important for sign language)\n",
       "        for hand in ['left_hand', 'right_hand']:\n",
       "            if hand in keypoints and np.any(keypoints[hand]):\n",
       "                # Flatten 3D coordinates to 1D array\n",
       "                hand_features = keypoints[hand].flatten()\n",
       "                frame_features.append(hand_features)\n",
       "            else:\n",
       "                # If hand not detected, use zeros\n",
       "                frame_features.append(np.zeros(21 * 3))\n",
       "        \n",
       "        # Process pose (smaller subset of pose landmarks relevant to signing)\n",
       "        if 'pose' in keypoints and np.any(keypoints['pose']):\n",
       "            # Extract only upper body landmarks (indexes 0-11 typically)\n",
       "            upper_body = keypoints['pose'][:12].flatten()\n",
       "            frame_features.append(upper_body)\n",
       "        else:\n",
       "            frame_features.append(np.zeros(12 * 3))\n",
       "        \n",
       "        # Concatenate all features for this frame\n",
       "        frame_features = np.concatenate(frame_features)\n",
       "        features.append(frame_features)\n",
       "    \n",
       "    # Convert to numpy array\n",
       "    features = np.array(features)\n",
       "    \n",
       "    return features\n",
       "\n",
       "# Example usage (with our placeholder data)\n",
       "example_sequence = [example_keypoints] * 5  # 5 frames of the same keypoints\n",
       "features = extract_features(example_sequence)\n",
       "\n",
       "# Print feature shape\n",
       "if features is not None:\n",
       "    print(f\"Feature shape: {features.shape}\")\n",
       "    print(f\"This represents {features.shape[0]} frames, each with {features.shape[1]} features.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Next Steps\n",
       "\n",
       "After exploring the data and extracting features, the next steps would be:\n",
       "\n",
       "1. **Data Collection**: Gather or download sign language datasets\n",
       "2. **Data Preprocessing**: Process videos to extract keypoints\n",
       "3. **Feature Engineering**: Extract relevant features from keypoints\n",
       "4. **Model Training**: Train a sign recognition model (e.g., LSTM, GRU, or Transformer)\n",
       "5. **Model Evaluation**: Evaluate model performance on validation data\n",
       "6. **Model Integration**: Integrate the trained model into the SignSpeak application\n",
       "\n",
       "These steps will be covered in separate notebooks."
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }